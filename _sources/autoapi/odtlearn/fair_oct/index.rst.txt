:py:mod:`odtlearn.fair_oct`
===========================

.. py:module:: odtlearn.fair_oct


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   odtlearn.fair_oct.FairConstrainedOCT
   odtlearn.fair_oct.FairSPOCT
   odtlearn.fair_oct.FairCSPOCT
   odtlearn.fair_oct.FairPEOCT
   odtlearn.fair_oct.FairEOppOCT
   odtlearn.fair_oct.FairEOddsOCT
   odtlearn.fair_oct.FairOCT




.. py:class:: FairConstrainedOCT(solver: str, positive_class: int, _lambda: float, obj_mode: str, fairness_bound: float, depth: int, time_limit: int, num_threads: None, verbose: bool)


   Bases: :py:obj:`odtlearn.constrained_oct.ConstrainedOCT`

   
   Base class for fair constrained optimal classification trees.

   This class extends the :mod:`ConstrainedOCT <odtlearn.constrained_oct.ConstrainedOCT>` class
   and provides a framework for implementing
   fair constrained optimal classification trees. It includes methods for adding fairness
   constraints, extracting metadata from the input data, and defining the objective function.

   :Parameters:

       **solver** : str
           The name of the solver to use for solving the MIP problem.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **_lambda** : float
           The regularization parameter in the objective. Must be in the interval [0, 1).

       **obj_mode** : {'acc', 'balance', 'weighted'}, optional (default='acc')
           The objective mode to use.
           'acc' for accuracy, 'balance' for balanced accuracy, 'weighted' for user-defined weights.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **depth** : int
           The maximum depth of the tree.

       **time_limit** : int
           The time limit (in seconds) for solving the MIP problem.

       **num_threads** : int, optional
           The number of threads to use for solving the MIP problem. If None, all available threads are used.

       **verbose** : bool, optional
           Whether to display verbose output during the solving process.









   .. rubric:: Notes

   This is a base class and should not be instantiated directly. Instead, use one of the
   derived classes that implement a specific fairness constraint, such as
   :mod:`FairSPOCT <odtlearn.fair_oct.FairSPOCT>`,
   :mod:`FairCSPOCT <odtlearn.fair_oct.FairCSPOCT>`,
   :mod:`FairPEOCT <odtlearn.fair_oct.FairPEOCT>`,
   :mod:`FairEOppOCT <odtlearn.fair_oct.FairEOppOCT>`,
   or :mod:`FairEOddsOCT <odtlearn.fair_oct.FairEOddsOCT>`.

   The :meth:`fit <odtlearn.fair_oct.FairConstrainedOCT.fit>` method expects the input data `X`,
   target labels `y`, protected features
   `protect_feat`, and legitimate factors `legit_factor` (if applicable) to be provided.
   The protected features should be binary-valued, and the legitimate factors should be
   numeric.

   The :meth:`predict <odtlearn.fair_oct.FairConstrainedOCT.predict>` method expects the input data
   `X` to have the same columns as the data
   used for fitting the model.



   :Attributes:

       **_obj_mode** : str
           The objective mode used for learning the optimal tree. Must be either 'acc' or 'balance'.

       **_positive_class** : int
           The value of the positive class label.

       **_fairness_bound** : float
           The bound of the fairness constraint. Must be in the interval (0, 1].

       **_protect_feat_col_labels** : list of str
           The column labels of the protected features.

       **_protect_feat_col_dtypes** : list of dtype
           The data types of the protected feature columns.

   .. rubric:: Methods



   ==============================================  ==========
   **_add_fairness_constraint(p_df, p_prime_df)**  Add the fairness constraint to the MIP problem for the given protected groups.  
        **_extract_metadata(X, y, protect_feat)**  Extract metadata from the input data.  
                          **_define_objective()**  Define the objective function for the MIP problem.  
        **fit(X, y, protect_feat, legit_factor)**  Fit the fair constrained optimal classification tree on the given data.  
                                   **predict(X)**  Predict the class labels for the given input data using the fitted model.  
   ==============================================  ==========

   ..
       !! processed by numpydoc !!
   .. py:method:: fit(X: numpy.ndarray, y: numpy.ndarray, protect_feat: numpy.ndarray, legit_factor: numpy.ndarray, weights: None = None) -> Union[FairCSPOCT, FairSPOCT, FairEOddsOCT, FairEOppOCT, FairPEOCT]

      
      Fit the Fair Constrained Optimal Classification Tree (FairConstrainedOCT) model to the given training data.


      :Parameters:

          **X** : array-like of shape (n_samples, n_features)
              The training input samples. Each feature should be binary (0 or 1).

          **y** : array-like of shape (n_samples,)
              The target values (class labels) for the training samples.

          **protect_feat** : array-like of shape (n_samples, n_protected_features)
              The protected feature columns (e.g., race, gender). Can have one or more columns.
              Each protected feature should be binary (0 or 1).

          **legit_factor** : array-like of shape (n_samples,)
              The legitimate factor column (e.g., prior number of criminal acts).
              This should be a numeric column.

          **weights** : array-like of shape (n_samples,), optional (default=None)
              Sample weights. If None, then samples are equally weighted when obj_mode is 'acc',
              or weights are automatically calculated when obj_mode is 'balance'.
              Must be provided when obj_mode is 'weighted'.

      :Returns:

          **self** : object
              Returns self.




      :Raises:

          ValueError
              If X or protect_feat contains non-binary values, or if inputs have inconsistent numbers of samples.
              Also raised if weights are not provided when obj_mode is 'weighted', or if the number
              of weights doesn't match the number of samples.

          AssertionError
              If the fairness bound is not in the range (0, 1].




      .. rubric:: Notes

      This method fits the FairConstrainedOCT model using mixed-integer optimization while
      considering fairness constraints. It sets up the optimization problem, solves it, and stores the results.

      The fairness constraints are applied based on the specific fairness metric defined in the subclass
      (e.g., Statistical Parity, Conditional Statistical Parity, Predictive Equality, or Equal Opportunity).

      The optimization problem aims to maximize accuracy (or balanced accuracy, depending on the obj_mode)
      while satisfying the fairness constraints within the specified fairness_bound.

      The resulting tree structure is stored in the model and can be used for prediction or visualization.

      The behavior of this method depends on the `obj_mode` specified during initialization:
      - If obj_mode is 'acc', equal weights are used (weights parameter is ignored).
      - If obj_mode is 'balance', weights are automatically calculated to balance class importance.
      - If obj_mode is 'weighted', the provided weights are used.

      When obj_mode is not 'weighted' and weights are provided, a warning is issued and the weights are ignored.


      .. rubric:: Examples

      >>> from odtlearn.fair_oct import FairConstrainedOCT
      >>> import numpy as np
      >>> X = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])
      >>> y = np.array([0, 1, 1, 0])
      >>> protect_feat = np.array([[1], [0], [1], [0]])
      >>> legit_factor = np.array([0.1, 0.2, 0.3, 0.4])
      >>> model = FairConstrainedOCT(solver="cbc", positive_class=1, depth=2, fairness_bound=0.1)
      >>> model.fit(X, y, protect_feat, legit_factor)



      ..
          !! processed by numpydoc !!

   .. py:method:: predict(X: Union[pandas.core.frame.DataFrame, numpy.ndarray]) -> numpy.ndarray

      
      Predict class labels for samples in X using the fitted Fair Constrained Optimal Classification Tree model.


      :Parameters:

          **X** : array-like of shape (n_samples, n_features)
              The input samples for which to make predictions. Each feature should be binary (0 or 1).

      :Returns:

          **y_pred** : ndarray of shape (n_samples,)
              The predicted class labels for each sample in X.




      :Raises:

          NotFittedError
              If the model has not been fitted yet.

          ValueError
              If X contains non-binary values or has a different number of features than the training data.




      .. rubric:: Notes

      This method uses the fair decision tree learned during the fit process to classify new samples.
      It traverses the tree for each sample in X, following the branching decisions until
      reaching a leaf node, and returns the corresponding class prediction.

      The predictions made by this method satisfy the fairness constraints that were imposed
      during the training process. However, note that the fairness guarantees only hold for the
      distribution of the training data. When applying the model to new data with a different
      distribution, the fairness properties may not be preserved.


      .. rubric:: Examples

      >>> from odtlearn.fair_oct import FairSPOCT
      >>> import numpy as np
      >>> X_train = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])
      >>> y_train = np.array([0, 1, 1, 0])
      >>> protect_feat = np.array([0, 1, 1, 0])
      >>> legit_factor = np.array([0, 1, 0, 1])
      >>> clf = FairSPOCT(solver="cbc", positive_class=1, depth=2, fairness_bound=0.1)
      >>> clf.fit(X_train, y_train, protect_feat, legit_factor)
      >>> X_test = np.array([[1, 1], [0, 0]])
      >>> y_pred = clf.predict(X_test)
      >>> print(y_pred)
      [1 0]



      ..
          !! processed by numpydoc !!


.. py:class:: FairSPOCT(solver: str, positive_class: int, depth: int = 1, time_limit: int = 60, _lambda: float = 0, obj_mode: str = 'acc', fairness_bound: float = 1, num_threads: Union[None, int] = None, verbose: bool = False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring statistical parity (SP) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode** : {'acc', 'balance', 'weighted'}, optional (default='acc')
           The objective mode to use.
           'acc' for accuracy, 'balance' for balanced accuracy, 'weighted' for user-defined weights.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!
   .. py:method:: calc_metric(protect_feat: Union[pandas.core.frame.DataFrame, numpy.ndarray], y: Union[pandas.core.series.Series, numpy.ndarray])

      
      Calculate the statistical parity metric for the given data.


      :Parameters:

          **protect_feat** : array-like of shape (n_samples, n_protected_features)
              The protected feature columns (e.g., race, gender). Can have one or more columns.

          **y** : array-like of shape (n_samples,)
              The target values or predicted values.

      :Returns:

          **sp_dict** : dict
              A dictionary with key (p,t) and value P(Y=t|P=p), where p is a protected level and t is an outcome value.








      .. rubric:: Notes

      This method calculates the statistical parity metric, which measures the difference in prediction rates
      across different protected groups.





      ..
          !! processed by numpydoc !!


.. py:class:: FairCSPOCT(solver: str, positive_class: int, depth: int = 1, time_limit: int = 60, _lambda: float = 0, obj_mode: str = 'acc', fairness_bound: float = 1, num_threads: Union[None, int] = None, verbose: bool = False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring conditional statistical parity (CSP) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode** : {'acc', 'balance', 'weighted'}, optional (default='acc')
           The objective mode to use.
           'acc' for accuracy, 'balance' for balanced accuracy, 'weighted' for user-defined weights.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!
   .. py:method:: calc_metric(protect_feat: Union[pandas.core.frame.DataFrame, numpy.ndarray], legit_factor: Union[pandas.core.frame.DataFrame, numpy.ndarray], y: Union[pandas.core.series.Series, numpy.ndarray])

      
      Calculate the conditional statistical parity metric for the given data.


      :Parameters:

          **protect_feat** : array-like of shape (n_samples, n_protected_features)
              The protected feature columns (e.g., race, gender). Can have one or more columns.

          **legit_factor** : array-like of shape (n_samples,)
              The legitimate factor column (e.g., prior number of criminal acts).

          **y** : array-like of shape (n_samples,)
              The target values or predicted values.

      :Returns:

          **csp_dict** : dict
              A dictionary with key (p, f, t) and value P(Y=t|P=p, L=f), where p is a protected level,
              t is an outcome value, and f is the value of the legitimate feature.








      .. rubric:: Notes

      This method calculates the conditional statistical parity metric, which measures the difference in
      prediction rates across different protected groups, conditioned on the legitimate factor.





      ..
          !! processed by numpydoc !!


.. py:class:: FairPEOCT(solver: str, positive_class: int, depth: int = 1, time_limit: int = 60, _lambda: float = 0, obj_mode: str = 'acc', fairness_bound: float = 1, num_threads: Union[None, int] = None, verbose: bool = False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring predictive equity (PE) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode: str, default="acc"**
           The objective should be used to learn an optimal decision tree.
           The two options are "acc" and "balance".
           The accuracy objective attempts to maximize prediction accuracy while the
           balance objective aims to learn a balanced optimal decision
           tree to better generalize to our of sample data.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!
   .. py:method:: calc_metric(protect_feat: Union[pandas.core.frame.DataFrame, numpy.ndarray], y: Union[pandas.core.series.Series, numpy.ndarray], y_pred: Union[pandas.core.series.Series, numpy.ndarray])

      
      Calculate the predictive equality metric for the given data.


      :Parameters:

          **protect_feat** : array-like of shape (n_samples, n_protected_features)
              The protected feature columns (e.g., race, gender). Can have one or more columns.

          **y** : array-like of shape (n_samples,)
              The true target values.

          **y_pred** : array-like of shape (n_samples,)
              The predicted values.

      :Returns:

          **eq_dict** : dict
              A dictionary with key (p, t, t_pred) and value P(Y_pred=t_pred|P=p, Y=t), where p is a protected level,
              t is a true outcome value, and t_pred is a predicted outcome value.








      .. rubric:: Notes

      This method calculates the predictive equality metric, which measures the difference in
      false positive rates across different protected groups.





      ..
          !! processed by numpydoc !!


.. py:class:: FairEOppOCT(solver: str, positive_class: int, depth: int = 1, time_limit: int = 60, _lambda: float = 0, obj_mode: str = 'acc', fairness_bound: float = 1, num_threads: Union[None, int] = None, verbose: bool = False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring equality of opportunity (EOpp) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode: str, default="acc"**
           The objective should be used to learn an optimal decision tree.
           The two options are "acc" and "balance".
           The accuracy objective attempts to maximize prediction accuracy while the
           balance objective aims to learn a balanced optimal decision
           tree to better generalize to our of sample data.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!
   .. py:method:: calc_metric()
      :abstractmethod:



.. py:class:: FairEOddsOCT(solver: str, positive_class: int, depth: int = 1, time_limit: int = 60, _lambda: float = 0, obj_mode: str = 'acc', fairness_bound: float = 1, num_threads: Union[None, int] = None, verbose: bool = False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring equal oddts (EOdds) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode: str, default="acc"**
           The objective should be used to learn an optimal decision tree.
           The two options are "acc" and "balance".
           The accuracy objective attempts to maximize prediction accuracy while the
           balance objective aims to learn a balanced optimal decision
           tree to better generalize to our of sample data.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!

.. py:class:: FairOCT(solver, positive_class, _lambda=0, depth=1, obj_mode='acc', fairness_type=None, fairness_bound=1, time_limit=60, num_threads=None, verbose=False)


   Bases: :py:obj:`odtlearn.flow_oct_ms.FlowOCTMultipleSink`

   
   An optimal and fair classification tree fitted on a given binary-valued
   data set. The fairness criteria enforced in the training step is one of statistical parity (SP),
   conditional statistical parity (CSP), predictive equality (PE),
   equal opportunity (EOpp) or equalized odds (EOdds).


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default= 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default= 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads

       **fairness_type: [None, 'SP', 'CSP', 'PE', 'EOpp', 'EOdds'], default=None**
           The type of fairness criteria that we want to enforce

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced














   ..
       !! processed by numpydoc !!
   .. py:method:: fit(X, y, protect_feat, legit_factor)

      
      Fit the FairOCT model to the given training data.


      :Parameters:

          **X** : array-like of shape (n_samples, n_features)
              The training input samples. Each feature should be binary (0 or 1).

          **y** : array-like of shape (n_samples,)
              The target values (class labels) for the training samples.

          **protect_feat** : array-like of shape (n_samples, n_protected_features)
              The protected feature columns (e.g., race, gender). Can have one or more columns.

          **legit_factor** : array-like of shape (n_samples,)
              The legitimate factor column (e.g., prior number of criminal acts).

      :Returns:

          **self** : object
              Returns self.




      :Raises:

          ValueError
              If X contains non-binary values or if inputs have inconsistent numbers of samples.




      .. rubric:: Notes

      This method fits the FairOCT model using mixed-integer optimization while
      considering fairness constraints. It sets up the optimization problem,
      solves it, and stores the results.





      ..
          !! processed by numpydoc !!

   .. py:method:: predict(X)

      
      Predict class labels for samples in X using the fitted FairOCT model.


      :Parameters:

          **X** : array-like of shape (n_samples, n_features)
              The input samples for which to make predictions. Each feature should be binary (0 or 1).

      :Returns:

          **y_pred** : ndarray of shape (n_samples,)
              The predicted class labels for each sample in X.




      :Raises:

          NotFittedError
              If the model has not been fitted yet.

          ValueError
              If X contains non-binary values or has a different number of features than the training data.




      .. rubric:: Notes

      This method uses the fair decision tree learned during the fit process to classify new samples.
      It traverses the tree for each sample in X, following the branching decisions until
      reaching a leaf node, and returns the corresponding class prediction.





      ..
          !! processed by numpydoc !!

   .. py:method:: get_SP(protect_feat, y)

      
      This function returns the statistical parity value for any given protected level and outcome value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns
      :param y: array-like, shape (n_samples,)
              The target values (class labels in classification).

      :return sp_dict: a dictionary with key =(p,t) and value = P(Y=t|P=p)
      where p is a protected level and t is an outcome value















      ..
          !! processed by numpydoc !!

   .. py:method:: get_CSP(protect_feat, legit_factor, y)

      
      This function returns the conditional statistical parity value for any given
      protected level, legitimate feature value and outcome value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns
      :param legit_fact: array-like, shape (n_samples,)
          The legitimate factor column(e.g., prior number of criminal acts)
      :param y: array-like, shape (n_samples,)
              The target values (class labels in classification).

      :return csp_dict: a dictionary with key =(p, f, t) and value = P(Y=t|P=p, L=f) where p is a protected level
                        and t is an outcome value and l is the value of the legitimate feature















      ..
          !! processed by numpydoc !!

   .. py:method:: get_EqOdds(protect_feat, y, y_pred)

      
      This function returns the false positive and true positive rate value
      for any given protected level, outcome value and prediction value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns

      :param y: array-like, shape (n_samples,)
              The true target values (class labels in classification).
      :param y_pred: array-like, shape (n_samples,)
              The predicted values (class labels in classification).

      :return eq_dict: a dictionary with key =(p, t, t_pred) and value = P(Y_pred=t_pred|P=p, Y=t)















      ..
          !! processed by numpydoc !!

   .. py:method:: get_CondEqOdds(protect_feat, legit_factor, y, y_pred)

      
      This function returns the conditional false negative and true positive rate value
      for any given protected level, outcome value, prediction value and legitimate feature value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns
      :param legit_factor: array-like, shape (n_samples,)
          The legitimate factor column(e.g., prior number of criminal acts)

      :param y: array-like, shape (n_samples,)
              The true target values (class labels in classification).
      :param y_pred: array-like, shape (n_samples,)
              The predicted values (class labels in classification).

      :return ceq_dict: a dictionary with key =(p, f, t, t_pred) and value = P(Y_pred=t_pred|P=p, Y=t, L=f)















      ..
          !! processed by numpydoc !!

   .. py:method:: fairness_metric_summary(metric, new_data=None)

      
      Summarize the specified fairness metric for the fitted model.


      :Parameters:

          **metric** : str
              The name of the fairness metric to summarize. Must be one of 'SP', 'CSP', 'PE', or 'CPE'.

          **new_data** : array-like of shape (n_samples,), optional
              The new predicted data to use for calculating the fairness metric. If None, the predict method
              is called on the training data to obtain the predicted values. Default is None.

      :Returns:

          None
              The method prints the fairness metric summary as a pandas DataFrame.




      :Raises:

          ValueError
              If the specified metric is not one of the supported options.




      .. rubric:: Notes

      This method summarizes the specified fairness metric for the fitted model. The supported fairness metrics are:
      - 'SP': Statistical Parity
      - 'CSP': Conditional Statistical Parity
      - 'PE': Predictive Equality
      - 'CPE': Conditional Predictive Equality

      The method checks if the model has been fitted and raises an error if not. If `new_data` is not provided,
      the predict method is called on the training data to obtain the predicted values.

      The fairness metric summary is printed as a pandas DataFrame, showing the metric values for each
      combination of protected attribute, legitimate factor (if applicable), true label, and predicted label
      (if applicable), depending on the selected metric.


      .. rubric:: Examples

      >>> model.fit(X_train, y_train, protect_feat_train, legit_factor_train)
      >>> model.fairness_metric_summary('SP')
                  (p,y)  P(Y=y|P=p)
      0     (Male, False)    0.752475
      1      (Male, True)    0.247525
      2   (Female, False)    0.742574
      3    (Female, True)    0.257426



      ..
          !! processed by numpydoc !!


