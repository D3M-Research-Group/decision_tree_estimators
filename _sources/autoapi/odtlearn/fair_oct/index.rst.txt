:py:mod:`odtlearn.fair_oct`
===========================

.. py:module:: odtlearn.fair_oct


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   odtlearn.fair_oct.FairConstrainedOCT
   odtlearn.fair_oct.FairSPOCT
   odtlearn.fair_oct.FairCSPOCT
   odtlearn.fair_oct.FairPEOCT
   odtlearn.fair_oct.FairEOppOCT
   odtlearn.fair_oct.FairEOddsOCT
   odtlearn.fair_oct.FairOCT




.. py:class:: FairConstrainedOCT(solver, _lambda, depth, time_limit, num_threads, verbose)


   Bases: :py:obj:`odtlearn.constrained_oct.ConstrainedOCT`

   
   Base class for fair constrained optimal classification trees.

   This class extends the :mod:`ConstrainedOCT <odtlearn.constrained_oct.ConstrainedOCT>` class
   and provides a framework for implementing
   fair constrained optimal classification trees. It includes methods for adding fairness
   constraints, extracting metadata from the input data, and defining the objective function.

   :Parameters:

       **solver** : str
           The name of the solver to use for solving the MIP problem.

       **_lambda** : float
           The regularization parameter in the objective. Must be in the interval [0, 1).

       **depth** : int
           The maximum depth of the tree.

       **time_limit** : int
           The time limit (in seconds) for solving the MIP problem.

       **num_threads** : int, optional
           The number of threads to use for solving the MIP problem. If None, all available threads are used.

       **verbose** : bool, optional
           Whether to display verbose output during the solving process.









   .. rubric:: Notes

   This is a base class and should not be instantiated directly. Instead, use one of the
   derived classes that implement a specific fairness constraint, such as
   :mod:`FairSPOCT <odtlearn.fair_oct.FairSPOCT>`,
   :mod:`FairCSPOCT <odtlearn.fair_oct.FairCSPOCT>`,
   :mod:`FairPEOCT <odtlearn.fair_oct.FairPEOCT>`,
   :mod:`FairEOppOCT <odtlearn.fair_oct.FairEOppOCT>`,
   or :mod:`FairEOddsOCT <odtlearn.fair_oct.FairEOddsOCT>`.

   The :meth:`fit <odtlearn.fair_oct.FairConstrainedOCT.fit>` method expects the input data `X`,
   target labels `y`, protected features
   `protect_feat`, and legitimate factors `legit_factor` (if applicable) to be provided.
   The protected features should be binary-valued, and the legitimate factors should be
   numeric.

   The :meth:`predict <odtlearn.fair_oct.FairConstrainedOCT.predict>` method expects the input data
   `X` to have the same columns as the data
   used for fitting the model.



   :Attributes:

       **_obj_mode** : str
           The objective mode used for learning the optimal tree. Must be either 'acc' or 'balance'.

       **_positive_class** : int
           The value of the positive class label.

       **_fairness_bound** : float
           The bound of the fairness constraint. Must be in the interval (0, 1].

       **_protect_feat_col_labels** : list of str
           The column labels of the protected features.

       **_protect_feat_col_dtypes** : list of dtype
           The data types of the protected feature columns.

   .. rubric:: Methods



   ============================================================================================  ==========
                                                 **_add_fairness_constraint(p_df, p_prime_df)**  Add the fairness constraint to the MIP problem for the given protected groups.  
                                                      **_extract_metadata(X, y, protect_feat)**  Extract metadata from the input data.  
                                                                        **_define_objective()**  Define the objective function for the MIP problem.  
   **:meth:`fit <odtlearn.fair_oct.FairConstrainedOCT.fit>`(X, y, protect_feat, legit_factor)**  Fit the fair constrained optimal classification tree on the given data.  
                          **:meth:`predict <odtlearn.fair_oct.FairConstrainedOCT.predict>`(X)**  Predict the class labels for the given input data using the fitted model.  
   ============================================================================================  ==========

   ..
       !! processed by numpydoc !!
   .. py:method:: fit(X, y, protect_feat, legit_factor)

      



      :Parameters:

          **X** : {array-like, sparse matrix}, shape (n_samples, n_features)
              The training input samples.

          **y** : array-like, shape (n_samples,)
              The target values (class labels in classification).

          **protect_feat** : array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); Can have one or more columns

          **legit_factor** : array-like, shape (n_samples,)
              The legitimate factor column(e.g., prior number of criminal acts)

      :Returns:

          **self** : object
              Returns self.













      ..
          !! processed by numpydoc !!

   .. py:method:: predict(X)

      
      Classify test points using the FairTree classifier


      :Parameters:

          **X** : array-like, shape (n_samples, n_features)
              The input samples.

      :Returns:

          **y** : ndarray, shape (n_samples,)
              The label for each sample is the label of the closest sample
              seen during fit.













      ..
          !! processed by numpydoc !!


.. py:class:: FairSPOCT(solver, positive_class, depth=1, time_limit=60, _lambda=0, obj_mode='acc', fairness_bound=1, num_threads=None, verbose=False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring statistical parity (SP) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode: str, default="acc"**
           The objective should be used to learn an optimal decision tree.
           The two options are "acc" and "balance".
           The accuracy objective attempts to maximize prediction accuracy while the
           balance objective aims to learn a balanced optimal decision
           tree to better generalize to our of sample data.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!
   .. py:method:: calc_metric(protect_feat, y)

      
      This function returns the statistical parity value for any given protected level and outcome value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns
      :param y: array-like, shape (n_samples,)
              The target values (class labels in classification).

      :return sp_dict: a dictionary with key =(p,t) and value = P(Y=t|P=p)
      where p is a protected level and t is an outcome value















      ..
          !! processed by numpydoc !!


.. py:class:: FairCSPOCT(solver, positive_class, depth=1, time_limit=60, _lambda=0, obj_mode='acc', fairness_bound=1, num_threads=None, verbose=False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring conditional statistical parity (CSP) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode: str, default="acc"**
           The objective should be used to learn an optimal decision tree.
           The two options are "acc" and "balance".
           The accuracy objective attempts to maximize prediction accuracy while the
           balance objective aims to learn a balanced optimal decision
           tree to better generalize to our of sample data.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!
   .. py:method:: calc_metric(protect_feat, legit_factor, y)

      
      Returns the conditional statistical parity value for any given
      protected level, legitimate feature value and outcome value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns
      :param legit_fact: array-like, shape (n_samples,)
          The legitimate factor column(e.g., prior number of criminal acts)
      :param y: array-like, shape (n_samples,)
              The target values (class labels in classification).

      :return csp_dict: a dictionary with key =(p, f, t) and value = P(Y=t|P=p, L=f) where p is a protected level
                        and t is an outcome value and l is the value of the legitimate feature















      ..
          !! processed by numpydoc !!


.. py:class:: FairPEOCT(solver, positive_class, depth=1, time_limit=60, _lambda=0, obj_mode='acc', fairness_bound=1, num_threads=None, verbose=False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring predictive equity (PE) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode: str, default="acc"**
           The objective should be used to learn an optimal decision tree.
           The two options are "acc" and "balance".
           The accuracy objective attempts to maximize prediction accuracy while the
           balance objective aims to learn a balanced optimal decision
           tree to better generalize to our of sample data.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!
   .. py:method:: calc_metric(protect_feat, y, y_pred)

      
      This function returns the false positive and true positive rate value
      for any given protected level, outcome value and prediction value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns

      :param y: array-like, shape (n_samples,)
              The true target values (class labels in classification).
      :param y_pred: array-like, shape (n_samples,)
              The predicted values (class labels in classification).

      :return eq_dict: a dictionary with key =(p, t, t_pred) and value = P(Y_pred=t_pred|P=p, Y=t)















      ..
          !! processed by numpydoc !!


.. py:class:: FairEOppOCT(solver, positive_class, depth=1, time_limit=60, _lambda=0, obj_mode='acc', fairness_bound=1, num_threads=None, verbose=False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring equality of opportunity (EOpp) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode: str, default="acc"**
           The objective should be used to learn an optimal decision tree.
           The two options are "acc" and "balance".
           The accuracy objective attempts to maximize prediction accuracy while the
           balance objective aims to learn a balanced optimal decision
           tree to better generalize to our of sample data.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!
   .. py:method:: calc_metric()
      :abstractmethod:



.. py:class:: FairEOddsOCT(solver, positive_class, depth=1, time_limit=60, _lambda=0, obj_mode='acc', fairness_bound=1, num_threads=None, verbose=False)


   Bases: :py:obj:`FairConstrainedOCT`

   
   An optimal classification tree fit on a given binary-valued data set
   with a fairness side-constraint requiring equal oddts (EOdds) between protected groups.


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default = 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default = 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **obj_mode: str, default="acc"**
           The objective should be used to learn an optimal decision tree.
           The two options are "acc" and "balance".
           The accuracy objective attempts to maximize prediction accuracy while the
           balance objective aims to learn a balanced optimal decision
           tree to better generalize to our of sample data.

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads














   ..
       !! processed by numpydoc !!

.. py:class:: FairOCT(solver, positive_class, _lambda=0, depth=1, obj_mode='acc', fairness_type=None, fairness_bound=1, time_limit=60, num_threads=None, verbose=False)


   Bases: :py:obj:`odtlearn.flow_oct_ms.FlowOCTMultipleSink`

   
   An optimal and fair classification tree fitted on a given binary-valued
   data set. The fairness criteria enforced in the training step is one of statistical parity (SP),
   conditional statistical parity (CSP), predictive equality (PE),
   equal opportunity (EOpp) or equalized odds (EOdds).


   :Parameters:

       **solver: str**
           A string specifying the name of the solver to use
           to solve the MIP. Options are "Gurobi" and "CBC".
           If the CBC binaries are not found, Gurobi will be used by default.

       **positive_class** : int
           The value of the class label which is corresponding to the desired outcome

       **depth** : int, default= 1
           A parameter specifying the depth of the tree

       **time_limit** : int, default= 60
           The given time limit (in seconds) for solving the MIO problem

       **_lambda** : float, default= 0
           The regularization parameter in the objective. _lambda is in the interval [0,1)

       **num_threads: int, default=None**
           The number of threads the solver should use. If None, it will use all avaiable threads

       **fairness_type: [None, 'SP', 'CSP', 'PE', 'EOpp', 'EOdds'], default=None**
           The type of fairness criteria that we want to enforce

       **fairness_bound: float (0,1], default=1**
           The bound of the fairness constraint. The smaller the value the stricter
           the fairness constraint and 1 corresponds to no fairness constraint enforced














   ..
       !! processed by numpydoc !!
   .. py:method:: fit(X, y, protect_feat, legit_factor)

      



      :Parameters:

          **X** : {array-like, sparse matrix}, shape (n_samples, n_features)
              The training input samples.

          **y** : array-like, shape (n_samples,)
              The target values (class labels in classification).

          **protect_feat** : array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); Can have one or more columns

          **legit_factor** : array-like, shape (n_samples,)
              The legitimate factor column(e.g., prior number of criminal acts)

          **Returns**
              ..

          **-------**
              ..

          **self** : object
              Returns self.














      ..
          !! processed by numpydoc !!

   .. py:method:: predict(X)

      
      Classify test points using the FairTree classifier
      Parameters
      ----------
      X : array-like, shape (n_samples, n_features)
          The input samples.
      Returns
      -------
      y : ndarray, shape (n_samples,)
          The label for each sample is the label of the closest sample
          seen during fit.
















      ..
          !! processed by numpydoc !!

   .. py:method:: get_SP(protect_feat, y)

      
      This function returns the statistical parity value for any given protected level and outcome value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns
      :param y: array-like, shape (n_samples,)
              The target values (class labels in classification).

      :return sp_dict: a dictionary with key =(p,t) and value = P(Y=t|P=p)
      where p is a protected level and t is an outcome value















      ..
          !! processed by numpydoc !!

   .. py:method:: get_CSP(protect_feat, legit_factor, y)

      
      This function returns the conditional statistical parity value for any given
      protected level, legitimate feature value and outcome value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns
      :param legit_fact: array-like, shape (n_samples,)
          The legitimate factor column(e.g., prior number of criminal acts)
      :param y: array-like, shape (n_samples,)
              The target values (class labels in classification).

      :return csp_dict: a dictionary with key =(p, f, t) and value = P(Y=t|P=p, L=f) where p is a protected level
                        and t is an outcome value and l is the value of the legitimate feature















      ..
          !! processed by numpydoc !!

   .. py:method:: get_EqOdds(protect_feat, y, y_pred)

      
      This function returns the false positive and true positive rate value
      for any given protected level, outcome value and prediction value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns

      :param y: array-like, shape (n_samples,)
              The true target values (class labels in classification).
      :param y_pred: array-like, shape (n_samples,)
              The predicted values (class labels in classification).

      :return eq_dict: a dictionary with key =(p, t, t_pred) and value = P(Y_pred=t_pred|P=p, Y=t)















      ..
          !! processed by numpydoc !!

   .. py:method:: get_CondEqOdds(protect_feat, legit_factor, y, y_pred)

      
      This function returns the conditional false negative and true positive rate value
      for any given protected level, outcome value, prediction value and legitimate feature value

      :param protect_feat: array-like, shape (n_samples,1) or (n_samples, n_p)
              The protected feature columns (Race, gender, etc); We could have one or more columns
      :param legit_factor: array-like, shape (n_samples,)
          The legitimate factor column(e.g., prior number of criminal acts)

      :param y: array-like, shape (n_samples,)
              The true target values (class labels in classification).
      :param y_pred: array-like, shape (n_samples,)
              The predicted values (class labels in classification).

      :return ceq_dict: a dictionary with key =(p, f, t, t_pred) and value = P(Y_pred=t_pred|P=p, Y=t, L=f)















      ..
          !! processed by numpydoc !!

   .. py:method:: fairness_metric_summary(metric, new_data=None)

      
      Summarize the specified fairness metric for the fitted model.


      :Parameters:

          **metric** : str
              The name of the fairness metric to summarize. Must be one of 'SP', 'CSP', 'PE', or 'CPE'.

          **new_data** : array-like of shape (n_samples,), optional
              The new predicted data to use for calculating the fairness metric. If None, the predict method
              is called on the training data to obtain the predicted values. Default is None.

      :Returns:

          None
              The method prints the fairness metric summary as a pandas DataFrame.




      :Raises:

          ValueError
              If the specified metric is not one of the supported options.




      .. rubric:: Notes

      This method summarizes the specified fairness metric for the fitted model. The supported fairness metrics are:
      - 'SP': Statistical Parity
      - 'CSP': Conditional Statistical Parity
      - 'PE': Predictive Equality
      - 'CPE': Conditional Predictive Equality

      The method checks if the model has been fitted and raises an error if not. If `new_data` is not provided,
      the predict method is called on the training data to obtain the predicted values.

      The fairness metric summary is printed as a pandas DataFrame, showing the metric values for each
      combination of protected attribute, legitimate factor (if applicable), true label, and predicted label
      (if applicable), depending on the selected metric.


      .. rubric:: Examples

      >>> model.fit(X_train, y_train, protect_feat_train, legit_factor_train)
      >>> model.fairness_metric_summary('SP')
                  (p,y)  P(Y=y|P=p)
      0     (Male, False)    0.752475
      1      (Male, True)    0.247525
      2   (Female, False)    0.742574
      3    (Female, True)    0.257426



      ..
          !! processed by numpydoc !!


